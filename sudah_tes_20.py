# -*- coding: utf-8 -*-
"""sudah tes 20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QA1WI5i777FnWR9XNm4ckRw6Pf_S3_Wg
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Memuat data dari file yang diunggah
train_data_path = '/content/drive/MyDrive/skripsi/datatrain2.csv'
test_data_path = '/content/drive/MyDrive/skripsi/datatest2.csv'

# Membaca dataset
train_data = pd.read_csv(train_data_path)
test_data = pd.read_csv(test_data_path)

train_data.head(), test_data.head()

from sklearn.preprocessing import StandardScaler

# Menghapus kolom yang tidak diperlukan
train_features = train_data.drop(['NIM', 'Nama Mahasiswa', 'Target'], axis=1)
train_targets = train_data['Target']
test_features = test_data.drop(['NIM', 'Nama Mahasiswa', 'Target'], axis=1)
test_targets = test_data['Target']

# Normalisasi data
scaler = StandardScaler()
train_features_scaled = scaler.fit_transform(train_features)
test_features_scaled = scaler.transform(test_features)

# Menampilkan beberapa data yang telah dinormalisasi
pd.DataFrame(train_features_scaled, columns=train_features.columns).head(),
pd.DataFrame(test_features_scaled, columns=test_features.columns).head()

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import BatchNormalization

# Membangun model neural network dengan dropout dan l2 regularization
model = Sequential([
    Dense(10, input_dim=train_features_scaled.shape[1], activation='relu'),
    BatchNormalization(),
    Dropout(0.5),  # Meningkatkan Dropout
    Dense(20, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),  # Meningkatkan Dropout
    Dense(1, activation='sigmoid')
])
optimizer = Adam(learning_rate=0.0001)  # Mengurangi learning rate lebih lanjut
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                                                 patience=5, min_lr=0.00001)
history = model.fit(
    train_features_scaled, train_targets, epochs=500, batch_size=16,
    verbose=1, validation_split=0.2,
    callbacks=[EarlyStopping(monitor='val_loss', patience=20,
                             restore_best_weights=True), reduce_lr]
)

# Evaluasi model
loss, accuracy = model.evaluate(test_features_scaled, test_targets)
print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')

import matplotlib.pyplot as plt

plt.figure(figsize=(14, 5))

# Plot training & validation accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training & validation loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Evaluasi model pada data test
loss, accuracy = model.evaluate(test_features_scaled, test_targets)
print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')

# Membuat prediksi pada data test
predictions = model.predict(test_features_scaled)
predicted_classes = (predictions > 0.5).astype(int).flatten()

results = pd.DataFrame({
    'NIM': test_data['NIM'],
    'Nama Mahasiswa': test_data['Nama Mahasiswa'],
    'Predicted Class': predicted_classes,
    'Target asli' : test_data['Target']
})

print(results)